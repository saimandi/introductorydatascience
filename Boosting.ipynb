{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boosting.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "Boosting is a sequential approach where the training of a model\n",
        "learner at a given step applies to \"residuals\" from the model fitted\n",
        "at the previous steps. This way, the current step improves on data\n",
        "points where the cumulative performance is not good. Boosting produces\n",
        "an ensemble model that in general less biased than the weak learners\n",
        "that compose it. \n",
        "\n",
        "Unlike bagging, boosting cannot be done in parallel.\n",
        "\n",
        "Boosting methods differ on how they create and aggregate the weak\n",
        "learners during the sequential process.\n",
        "\n",
        "See [Joseph Rocca's\n",
        "post](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205);\n",
        "[Jason Brownlee's\n",
        "blog](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/).\n"
      ],
      "metadata": {
        "id": "0J2m7_cEhotQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adaptive Boosting (AdaBoost)\n",
        "Adaptive boosting updates the weights attached to each of the training\n",
        "dataset observations. The ensemble model is a weighted sum of the weak\n",
        "learners. Instead of trying to solve it in one single shot (finding\n",
        "all the coefficients and weak learners that give the best overall\n",
        "additive model), an iterative optimisation process is\n",
        "much more tractable, even if it can lead to a sub-optimal solution.\n",
        "\n",
        "\n",
        "For illustration, consider a binary classification problem, with $N$\n",
        "observations and a given family of weak models. To initialize, all the\n",
        "observations have the same weights $1/N$. Repeat over each weak model:\n",
        "+ fit the best possible weak model with the current observations\n",
        "  weights;\n",
        "+ compute the value of the update coefficient that is some kind of\n",
        "  scalar evaluation metric of the weak learner that indicates how much\n",
        "  this weak learner should be taken into account into the ensemble\n",
        "  model;\n",
        "+ update the strong learner by adding the new weak learner multiplied\n",
        "  by its update coefficient;\n",
        "+ compute new observations weights that express which observations we \n",
        "  would like to focus on at the next iteration (weights of observations \n",
        "  wrongly predicted by the aggregated model increase and weights of \n",
        "  the correctly predicted observations decrease)."
      ],
      "metadata": {
        "id": "clRrSspihxWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "Gradient boosting updates the dataset in each step. It casts the\n",
        "problem into a gradient descent one: at each iteration we fit a weak\n",
        "learner to the negative gradients of the current fitting error\n",
        "(pseudo-residuals) with respect to the current ensemble model. In the\n",
        "regression setting, the negative gradient is the residual from the\n",
        "current ensemble model.\n",
        "\n",
        "For illustration, continue with the binary classification\n",
        "problem. Initialize the pseudo-residuals as the observation outcomes.\n",
        "Repeat over the following steps:\n",
        "+ fit the best possible weak learner to pseudo-residuals (approximate\n",
        "  the negative gradients with respect to the current strong learner);\n",
        "+ compute the value of the optimal step size that defines by how much\n",
        "  we update the ensemble model in the direction of the new weak\n",
        "  learner;\n",
        "+ update the ensemble model by adding the new weak learner multiplied\n",
        "  by the step size (make a step of gradient descent);\n",
        "+ compute new pseudo-residuals that indicate, for each observation, in\n",
        "  which direction we would like to update next the ensemble model\n",
        "  predictions.\n",
        "\n",
        "Gradient boosting uses a gradient descent approach and can be easily\n",
        "adapted to a large number of loss functions. It can be considered as a\n",
        "generalization of adaboost to arbitrary differentiable loss functions.\n"
      ],
      "metadata": {
        "id": "pN1hqcqDJqSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Stochastic gradient boosting\n",
        "\n",
        "Stochastic gradient boosting (SGD) is a stochastic version of gradient\n",
        "boosting. At each iteration, a subsample of the training data is drawn\n",
        "at random (without replacement) from the full training dataset. The\n",
        "randomly selected subsample is then used, instead of the full sample,\n",
        "to fit the base learner. It reduce the dependence between the trees\n",
        "in the sequence in gradient boosting models.\n",
        "\n",
        "\n",
        "\n",
        "#### Extreme gradient boosting\n",
        "Extreme gradient boosting is a specific implementation of the gradient\n",
        "boosting method which uses more accurate approximations to find the\n",
        "best tree model. It employs a number of nifty tricks that make it\n",
        "exceptionally successful, particularly with structured data. \n",
        "\n",
        "+ computing second-order gradients, i.e. second partial derivatives of\n",
        "  the loss function (similar to Newtonâ€™s method), which provides more\n",
        "  information about the direction of gradients and how to get to the\n",
        "  minimum of our loss function. While regular gradient boosting uses\n",
        "  the loss function of our base model (e.g. decision tree) as a proxy\n",
        "  for minimizing the error of the overall model, XGBoost uses the 2nd\n",
        "  order derivative as an approximation.\n",
        "+ advanced regularization (L1 & L2), which improves model\n",
        "  generalization.\n",
        "+ parallelized.\n",
        "\n"
      ],
      "metadata": {
        "id": "HFwktgnyJx_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting with NYC Crash Data Example\n",
        "We can demonstrate the implementation of AdaBoost, Stochastic Gradient Boost and XG Boost in Python using sklearn.\n",
        "An interesting metric to predict is whether or not the incident resulted in a casualty.\n",
        "\n",
        "We will use the following predictors to train our model.\n",
        "\n",
        "+ `hour`: the hour at which the crash occured\n",
        "+ `borough`: which borough the crash took place in\n",
        "+ `num_vehicles_involved`: number of vehicles involved in a crash\n",
        "+ `weekday`: day of week of the crash, where 0 represents Monday and 6 represents Sunday"
      ],
      "metadata": {
        "id": "1VQy8PHfKQlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODX_vhDKhnV8",
        "outputId": "caeeb666-cdc9-4da6-e003-cbf1d8c01692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from datetime import datetime\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/statds/ids-s22/main/notes/data/nyc_mv_collisions_202201.csv'\n",
        "nyc_crash = pd.read_csv(url)\n",
        "nyc_crash.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQfqx7b7BYF",
        "outputId": "1911c752-21d7-4f0d-ae14-cfdd27a60d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7659 entries, 0 to 7658\n",
            "Data columns (total 29 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   CRASH DATE                     7659 non-null   object \n",
            " 1   CRASH TIME                     7659 non-null   object \n",
            " 2   BOROUGH                        5025 non-null   object \n",
            " 3   ZIP CODE                       5025 non-null   float64\n",
            " 4   LATITUDE                       7097 non-null   float64\n",
            " 5   LONGITUDE                      7097 non-null   float64\n",
            " 6   LOCATION                       7097 non-null   object \n",
            " 7   ON STREET NAME                 5625 non-null   object \n",
            " 8   CROSS STREET NAME              3620 non-null   object \n",
            " 9   OFF STREET NAME                2034 non-null   object \n",
            " 10  NUMBER OF PERSONS INJURED      7659 non-null   int64  \n",
            " 11  NUMBER OF PERSONS KILLED       7659 non-null   int64  \n",
            " 12  NUMBER OF PEDESTRIANS INJURED  7659 non-null   int64  \n",
            " 13  NUMBER OF PEDESTRIANS KILLED   7659 non-null   int64  \n",
            " 14  NUMBER OF CYCLIST INJURED      7659 non-null   int64  \n",
            " 15  NUMBER OF CYCLIST KILLED       7659 non-null   int64  \n",
            " 16  NUMBER OF MOTORIST INJURED     7659 non-null   int64  \n",
            " 17  NUMBER OF MOTORIST KILLED      7659 non-null   int64  \n",
            " 18  CONTRIBUTING FACTOR VEHICLE 1  7615 non-null   object \n",
            " 19  CONTRIBUTING FACTOR VEHICLE 2  5624 non-null   object \n",
            " 20  CONTRIBUTING FACTOR VEHICLE 3  824 non-null    object \n",
            " 21  CONTRIBUTING FACTOR VEHICLE 4  225 non-null    object \n",
            " 22  CONTRIBUTING FACTOR VEHICLE 5  80 non-null     object \n",
            " 23  COLLISION_ID                   7659 non-null   int64  \n",
            " 24  VEHICLE TYPE CODE 1            7539 non-null   object \n",
            " 25  VEHICLE TYPE CODE 2            4748 non-null   object \n",
            " 26  VEHICLE TYPE CODE 3            752 non-null    object \n",
            " 27  VEHICLE TYPE CODE 4            207 non-null    object \n",
            " 28  VEHICLE TYPE CODE 5            78 non-null     object \n",
            "dtypes: float64(3), int64(9), object(17)\n",
            "memory usage: 1.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a casualty variable to determine how many people are killed or injured in a crash\n",
        "nyc_crash['casualty'] = nyc_crash.apply(lambda row: 1 if (row['NUMBER OF PERSONS KILLED'] + row['NUMBER OF PERSONS INJURED']) > 0 else 0, axis = 1)\n",
        "\n",
        "# Casting to category\n",
        "nyc_crash['casualty'] = nyc_crash['casualty'].astype('category')\n",
        "\n",
        "# Creating a hour variable to determine the hour at which the incident took place\n",
        "def get_hour(entry):\n",
        "  time = datetime.strptime(entry, \"%H:%M\")\n",
        "  return int(time.hour)\n",
        "\n",
        "nyc_crash['hour'] = nyc_crash['CRASH TIME'].apply(get_hour)\n",
        "\n",
        "# Creating a number of vehicles involved variable\n",
        "contributing_factors = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "                        'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4',\n",
        "                        'CONTRIBUTING FACTOR VEHICLE 5']\n",
        "\n",
        "nyc_crash['num_vehicles_involved'] = len(nyc_crash[contributing_factors].columns) - nyc_crash[\n",
        "                                         contributing_factors].isnull().sum(axis = 1)\n",
        "\n",
        "# Creating a day of the week variable where 0 represents Monday and 6 represents Sunday\n",
        "def get_weekday(entry):\n",
        "  entry = datetime.strptime(entry, \"%m/%d/%Y\")\n",
        "  return entry.weekday()\n",
        "  \n",
        "nyc_crash['weekday'] = nyc_crash['CRASH DATE'].apply(get_weekday)\n",
        "nyc_crash.rename(columns = {'BOROUGH': 'borough'}, inplace = True)\n",
        "nyc_crash.dropna(subset=['borough'], inplace=True)\n",
        "nyc_crash[['borough', 'hour', 'num_vehicles_involved', 'weekday']].isnull().sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33yazOAf9Rxc",
        "outputId": "cc7b507d-0e2f-4317-c7ae-1dcde32e1d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "borough                  0\n",
              "hour                     0\n",
              "num_vehicles_involved    0\n",
              "weekday                  0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = nyc_crash[['hour', 'num_vehicles_involved', 'weekday']] # Predictors\n",
        "labels = np.array(nyc_crash['casualty']) # Outcome to predict\n",
        "\n",
        "# In-place one-hot endcoding for borough variable\n",
        "pd.get_dummies(features)\n",
        "\n",
        "# Creating test and train sets with 20% of data going to test set\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "FfQIqc6sS2SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will initialize an Adaptive Boosting, Gradient Boosting, Stochastic Gradient Boosting, and XGBoosting classifiers. We can compare their accuracy score, which is the number of correctly classified observations / total number of observations."
      ],
      "metadata": {
        "id": "4vu7_ZXbBN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada_booster = AdaBoostClassifier(random_state=42)\n",
        "ada_booster.fit(x_train, y_train)\n",
        "ada_booster.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkSLeAeOv0ME",
        "outputId": "e6fcfba6-3317-4d2c-b37d-b114ca9cf023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7223880597014926"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_booster = GradientBoostingClassifier(random_state=42)\n",
        "gradient_booster.fit(x_train, y_train)\n",
        "gradient_booster.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAQoDYZvD-Oa",
        "outputId": "f9d4a060-97a1-493d-9e78-6c69daa7f0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7243781094527363"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stochastic_gradient_booster = GradientBoostingClassifier(subsample=0.5, random_state=42)\n",
        "stochastic_gradient_booster.fit(x_train, y_train)\n",
        "stochastic_gradient_booster.score(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NR0vACq4qqz",
        "outputId": "112f8a27-2ea4-428e-863a-ce98865f6d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.718407960199005"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xg_booster = XGBClassifier(random_state=42)\n",
        "xg_booster.fit(x_train, y_train)\n",
        "y_pred = xg_booster.predict(x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tih6p-0wEZSQ",
        "outputId": "31cab878-d19f-48a6-d9d0-0c4ab5201851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7203980099502487"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of these boosting algorithms present similar accuracy scores for this dataset. However, they all use different methods of computation. Perhaps for larger data sets and with more focused hyperparameter tuning, the difference between these algorithms would become more apparent."
      ],
      "metadata": {
        "id": "NsXHYQanF5qC"
      }
    }
  ]
}